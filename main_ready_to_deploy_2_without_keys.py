

from flask import Flask, render_template,jsonify,request
from flask_cors import CORS
from flask import Flask, render_template, url_for
import requests,openai,os
from langchain.chains import LLMChain
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA

from pinecone import Pinecone
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from openai import OpenAI

from langchain_core.prompts import PromptTemplate

from dotenv.main import load_dotenv
load_dotenv()


#DATA_PATH = "data/dataset"








app = Flask(__name__,static_folder='static',template_folder='templates')
CORS(app)





#template = """Answer the question in your own words..
#If you do not know the answer to the question, simply respond with "I don't know. Can you ask another question", and your response should be friendly.
#If questions are asked where there is no relevant context available, simply respond with "I can't find this information in given documents, Can you please repeat your question by providing more information."
#Here is Contex: {context}

template = """Answer the question in your own words based on given information. you have to provide answer at every cost as per your understanding.
Here is Information: {context}

{chat_history}
Human: {question}
Assistant:"""

prompt = PromptTemplate(
    input_variables=["context", "chat_history", "question"], template=template
)

memory = ConversationBufferMemory(memory_key="chat_history")

embeddings = OpenAIEmbeddings()

pc = Pinecone(api_key='9619b277-b411-43a6-be65-689f4fa2907e',
              #ssl_ca_certs='~/.mitmproxy/mitmproxy-ca.pem'
              )
print(pc.list_indexes())
index = pc.Index('michaeles')
print(index)
vectorstore = PineconeVectorStore(index=index, embedding=embeddings)
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold", search_kwargs={"score_threshold": 0.3}
)

qachat = ''







@app.route('/')
def index():
    global qachat
    llm = ChatOpenAI(temperature=0.6, model_name="gpt-4o")
    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer')
    qachat = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory,
        return_source_documents=True,
        combine_docs_chain_kwargs={'prompt': prompt}
        )

    return render_template('index.html')


@app.route('/data', methods=['POST'])
def get_data():
    global qachat
    if True:
        data = request.get_json()
        text=data.get('data')
        user_input = text
        print('************************')
        print(user_input)
        print('************************')
        response = qachat(user_input)
        print(response['answer'])
        model_reply = response['answer']
        #model_reply = model_reply.replace('*','')
        #model_reply = model_reply.replace('#','')
        
        return jsonify({"response":True,"message":model_reply})
        '''
        #if 'sorry' in model_reply or "I don't":
        #print(response['chat_history'])
        hist = str(response['chat_history'])
        query4gpt = f"""See this Answer, if it looks good, produce same response, if it says I don't know or I am sorry, I don't have information etc. then you should write answer of given question according to your knowledge,
        Question : {user_input}
        Answer to this question by first model : {model_reply}
        Chat history : {hist}
        """
        completion = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "you are helpful assistant and your task is to monitor the text that is given to you, you have to do this anonymously, that means no one should guess that you are altering response somehow, Its output of another model that is trained on pdf documents, if response is good, pass that response as it is, if it says something like sorry I don't know, or I am sorry I can't provide this information etc, then you should take over and provide answer by your knowledge, I will provide you three inputs, that are Question, it answer generated by first model and chat history."}, #You are a helpful assistant and your task is to provide refine output minimal way possible of first language model that is trained on custom data, you are second language model in this chain. first language model sometime produces silly response so you have to correct it if needed, If you recieve something like I don't know, or I can't find information, then you have to answer that question according to your understanding and on base of chat history that I will provide.
                    {"role": "user", "content":query4gpt }
              ]
            )
        model_reply = completion.choices[0].message.content
        print('>>>>>>>>>>>'+model_reply)
        '''
        




'''
@app.route('/')
def index():
    llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo")
    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer')

    qachat = ConversationalRetrievalChain.from_llm(
            llm=ChatOpenAI(temperature=0),
            memory=memory,
            retriever=retriever,
            return_source_documents=True
        )

    return render_template('index.html')

@app.route('/data', methods=['POST'])
def get_data():
    if True:

        data = request.get_json()
        text=data.get('data')
        user_input = text
        print('************************')
        print(user_input)
        print('************************')


        model_reply = qachat(user_input)['answer']
        return jsonify({"response":True,"message":model_reply})
'''

'''

        PROMPT_TEMPLATE = """
        Answer the question based only on the following context:
        {context}
        ------------
        Answer the question based on the above context: {question}
        """
        query_text = user_input
        try:
            results = vectorstore.similarity_search_with_relevance_scores(query_text, k=3)
            if len(results) == 0 or results[0][1] < 0.7:
                print(f"Unable to find matching results.")
                #return

            context_text = "\n\n---\n\n".join([doc.page_content for doc, _score in results])
            prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
            prompt = prompt_template.format(context=context_text, question=query_text)
            #print(prompt)


            response_text = model.predict(prompt)
            sources = [doc.metadata.get("source", None) for doc, _score in results]
            model_reply = f"{response_text}"
            print('##############################################################')
            print(model_reply)
            return jsonify({"response":True,"message":model_reply})
        except Exception as e:
            model_reply = str(e)
            return jsonify({"response":True,"message":model_reply})
'''

if __name__ == '__main__':
    app.run(debug=True)
